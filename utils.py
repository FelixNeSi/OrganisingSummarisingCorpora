import pandas as pd


def get_dataframe_from_csv(file_name):
    df = pd.read_csv(file_name)
    return df


def save_dataframe_to_csv(df, file_name, headers=[]):
    df.to_csv(file_name, headers=headers)


def calculate_mean_average_precision(predicted_keywords, true_keywords):
    all_precisions = []
    for i, keywords in enumerate(predicted_keywords):
        temp_precisions = []
        temp_correct = 0
        for j, kw in enumerate(keywords):
            if kw in true_keywords[i]:
                temp_correct += 1
                temp_precisions.append(temp_correct / (j + 1))
        if temp_correct == 0:
            all_precisions.append(0)
        else:
            all_precisions.append(sum(temp_precisions) / temp_correct)
    mean_average_precision = sum(all_precisions) / len(predicted_keywords)
    return mean_average_precision


def tensor_to_list(tensors):
    tensor_list = []
    for tensor in tensors:
        tensor_list.append(tensor)
    return tensor_list


def print_kwargs(**kwargs):
    print(kwargs['printt'])
    for x in kwargs['list']:
        print(x)
    for val in kwargs.values():
        print('VAVAV {}'.format(val))

# save_name = 'experiment/{}/{}-{}-{}-{}'.format('kdd-science', 'kdd-science', 'fast', 'allenai/scibert_scivocab_uncased'.replace('/', '-'), 'no-pre')
# print(save_name)
#
# x=['3', '2']
# df = pd.DataFrame(x)
# df.to_csv('{}.csv'.format(save_name))
# grouped_docs = ['ab', 'cd ', 'efg']
# temp_combined_doc = ''
# for doc in grouped_docs:
#     temp_combined_doc = temp_combined_doc + ' ' + doc
#
# quick = ' '.join(grouped_docs)
# print(temp_combined_doc)
# print(quick)
#
# print_kwargs(printt='PRINT THIS', list=['x', 'y', 'z'])\
# df = pd.read_csv("kw_extraction_all.csv")
# print(df['approach'])
# import pickle
# from yellowbrick.cluster import KElbowVisualizer
# from sklearn.cluster import KMeans
# import torch
#
# with open("Kravpivin2009_lda_25.pickle", 'rb') as handle:
#     Kravpivin_miniLM = pickle.load(handle)

# print(len(Kravpivin_miniLM))
#
# doc_embeddings = [t.numpy() for t in Kravpivin_miniLM]
# stacked = torch.squeeze(torch.stack(doc_embeddings))
#
# km = KMeans(random_state=42)
# visualizer = KElbowVisualizer(km, k=(2, 10))
#
# visualizer.fit(Kravpivin_miniLM[9].reshape(1, -1))  # Fit the data to the visualizer
# visualizer.show()

# precomputed! <class 'torch.Tensor'>
# array=[-6.47373348e-02  1.05761722e-01  1.96367167e-02 -5.67311086e-02
#   3.56736146e-02 -8.93668756e-02 -1.26369474e-02 -4.23799967e-03
#  -1.08910128e-02  5.48526607e-02  1.03682256e-03  1.33287096e-02
#   4.76458296e-02 -7.61505542e-03 -4.48435952e-04 -4.49398682e-02
#   5.38306637e-03  1.60352550e-02 -2.86135711e-02 -5.97864762e-02
#   7.32982066e-03 -7.80122727e-02 -6.41081557e-02  6.77800700e-02
#  -2.80426368e-02  3.29192840e-02 -9.34699364e-03 -4.90348525e-02
#   2.22580079e-02 -2.60718074e-02  5.40720858e-02 -4.03914397e-04
#  -8.12337641e-03 -1.39168533e-03  9.44033917e-03  4.73743044e-02
#   1.27265919e-02 -8.23556110e-02 -5.73495142e-02 -1.05729140e-01
#  -2.05022166e-03 -9.05726105e-03 -6.57636002e-02  4.00420604e-03
#   1.71362620e-03  3.99006866e-02  5.69888987e-02  2.24971157e-02
#  -5.28305061e-02 -4.15763929e-02  3.88140045e-02  1.64114833e-02
#  -6.43011853e-02  6.84968084e-02 -6.52012741e-03  3.30004841e-02
#   2.06845831e-02  5.68327354e-03 -7.93896094e-02  7.54473880e-02
#  -9.38761979e-02 -1.61109231e-02 -8.28312933e-02  1.72912702e-02
#  -6.91248616e-03  5.33647761e-02  8.56874064e-02  1.23639069e-02
#   6.87885145e-03  7.12258220e-02 -4.87714894e-02 -1.52586699e-02
#   5.37455007e-02  9.46578309e-02 -5.30168861e-02  6.40994981e-02
#   1.68144368e-02 -7.42610618e-02 -6.47640824e-02 -7.90333748e-02
#  -3.64891961e-02 -1.65770520e-02  1.24539332e-02 -2.13520098e-02
#   7.23553747e-02 -9.07480419e-02 -4.45638523e-02 -7.06639364e-02
#   6.32981509e-02  2.36408561e-02  3.44637707e-02  4.20421362e-02
#   4.42097187e-02  2.83027310e-02  4.66615409e-02  3.87731977e-02
#   5.46162911e-02  2.87224595e-02 -5.56188228e-04  7.34940246e-02
#   2.37332680e-03  7.67327175e-02 -3.38495418e-04 -6.60482049e-02
#   6.95955306e-02  1.40533522e-02  1.31233688e-02  9.74537432e-03
#  -2.33795810e-02 -2.68010572e-02 -6.41568098e-03  2.97858976e-02
#   7.78034776e-02  1.87250096e-02 -2.09308099e-02  7.01056570e-02
#  -2.50558499e-02  1.50719541e-03 -1.88233815e-02 -1.01630434e-01
#   6.48594135e-03 -3.16993929e-02  4.46136147e-02  1.16541218e-02
#   4.70213033e-03 -1.32729928e-03 -1.50389755e-02  4.82758926e-33
#  -1.41136218e-02  1.14714392e-01 -1.32033266e-02 -3.58966924e-03
#   1.81058757e-02  6.78163394e-02  1.03078306e-01  9.43763927e-03
#  -1.24214500e-01 -1.85073763e-02  3.44780944e-02 -6.12887777e-02
#   2.03215871e-02 -5.31642884e-03  6.38591945e-02 -4.27327938e-02
#   5.73993847e-02  5.98309301e-02  3.17194033e-03 -2.87476252e-03
#   2.59941015e-02 -2.73561552e-02 -3.82399037e-02 -5.53475954e-02
#   6.53887764e-02  1.28964717e-02 -3.62311043e-02 -8.33339989e-03
#   5.25598973e-03  1.27983922e-02 -4.00013402e-02 -1.17128031e-04
#  -3.08635086e-02  9.87535045e-02  7.14116618e-02 -1.50200753e-02
#   4.76929406e-03 -7.85974711e-02  1.33610740e-02 -9.05806050e-02
#   1.51143558e-02 -3.61406506e-04 -8.13677460e-02 -2.26333998e-02
#  -7.15540573e-02 -9.27530229e-02 -7.61933923e-02  2.55613420e-02
#   4.49012257e-02 -1.72874555e-02  1.15202539e-01  2.69960817e-02
#   9.70838498e-03 -1.04956934e-02 -3.04250838e-03 -7.37670958e-02
#   2.97962595e-02  3.93459164e-02  1.16972990e-01  1.72545716e-01
#  -5.03850589e-03  3.15320045e-02 -6.76648840e-02  3.18216197e-02
#  -1.32800220e-03  1.40458299e-02 -1.19573578e-01 -1.97831653e-02
#   3.39451134e-02  3.23384218e-02 -5.31145036e-02  5.45888953e-02
#   4.80158851e-02  3.93900461e-02  6.22993447e-02 -1.81358121e-02
#   3.18639278e-02 -6.31511584e-02 -3.05007305e-02 -7.58868232e-02
#   3.03660538e-02  7.35225677e-02  4.85374108e-02  9.53737646e-02
#   1.17597347e-02 -9.20242518e-02 -7.47111905e-03  9.29359943e-02
#  -6.13672994e-02 -2.31896099e-02 -1.01333624e-02 -5.70208691e-02
#   1.13291599e-01 -1.77209489e-02  1.30063267e-02 -3.60341291e-33
#  -1.01363752e-03  7.76921120e-03 -5.46484906e-03  6.93953335e-02
#  -3.45342532e-02 -1.51519999e-01  3.53616625e-02 -8.29278156e-02
#  -6.23589531e-02 -6.55825511e-02  4.04797904e-02 -2.80174762e-02
#   8.14123899e-02 -7.36625446e-03  8.45892057e-02  1.05017098e-02
#  -6.89685419e-02 -1.11435704e-01  5.98216150e-03  4.05503139e-02
#   5.02930433e-02  7.96796009e-02  1.36801667e-04  2.00323369e-02
#  -1.87191609e-02  3.98922861e-02 -4.11616638e-02  6.87310472e-02
#   4.70842645e-02 -7.32259126e-03 -3.47507969e-02 -1.08325714e-02
#  -7.48577947e-03 -6.88826963e-02 -1.44069726e-02 -5.40089654e-03
#  -3.08328518e-03  8.35525468e-02  4.07823827e-03  6.12100549e-02
#   7.48066753e-02 -3.90158556e-02 -5.72383329e-02  4.36186790e-02
#  -2.22089477e-02  3.88195291e-02 -4.53453436e-02  9.35771987e-02
#  -1.34030124e-02 -3.76720317e-02 -5.03652580e-02  1.88973770e-02
#   2.41267644e-02  1.88717199e-03  5.98948039e-02  2.69555300e-03
#  -5.42331226e-02  3.92377302e-02  2.19570771e-02 -6.12317696e-02
#   1.42885316e-02 -7.68746287e-02  5.95268197e-02  6.54579140e-03
#   6.37460127e-02  1.42642166e-02  1.40140567e-03 -2.02915110e-02
#   2.23744456e-02  8.96327384e-03  2.06034966e-02  5.20250387e-02
#  -5.40577434e-02  1.53990472e-02  6.95874691e-02 -3.90262380e-02
#   2.68704332e-02 -2.40145214e-02  5.66790923e-02  6.99044392e-02
#  -6.96003661e-02  5.61697893e-02 -1.22693386e-02 -6.31498694e-02
#  -4.28182185e-02  1.57406759e-02 -5.12319766e-02  3.21417488e-02
#  -6.98082447e-02 -2.41529569e-02 -1.74758732e-02  4.97008152e-02
#   3.43593024e-02  7.29738846e-02  1.71084031e-02 -4.84310227e-08
#   6.94396533e-03 -1.04672767e-01  4.48520407e-02  2.62256227e-02
#   7.38518536e-02 -3.19493748e-02 -3.08101922e-02 -4.86785779e-03
#   1.00202963e-01 -9.78118777e-02  2.40021925e-02  2.20334604e-02
#  -3.81436348e-02 -8.78612995e-02  1.70751642e-02  6.86849980e-03
#   6.67029293e-03 -5.19592091e-02 -9.71270129e-02 -5.33937290e-02
#  -6.82434589e-02  2.00395565e-03 -4.00532447e-02  5.17597236e-02
#   3.77116986e-02 -1.25167836e-02  1.20005809e-01  5.94968237e-02
#   5.46877906e-02 -1.19380252e-02 -7.57808611e-03 -2.17455290e-02
#   4.28058878e-02  4.39965492e-03  6.23324104e-02  1.08108893e-01
#   1.11654587e-02  7.35119507e-02  1.67796779e-02 -5.20795435e-02
#  -1.58575401e-02  2.15301774e-02 -6.02866709e-02  5.50308116e-02
#   1.40655739e-02 -2.47596204e-02  6.78121950e-03 -7.04175383e-02
#  -4.17721979e-02 -3.95008512e-02 -7.35248327e-02 -2.31140703e-02
#  -1.59011520e-02  3.96680683e-02 -1.26155928e-01  3.87336612e-02
#  -4.92280945e-02 -5.16609848e-02  1.87406428e-02  3.56347524e-02
#   4.55571637e-02 -5.09047124e-04  2.51912847e-02 -2.39392947e-02].
# Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.